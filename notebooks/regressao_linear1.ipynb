{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear 1: equações normais\n",
    "\n",
    "\n",
    "Dado o dataset $(\\mathbf{x}_{1}, y_{1}), \\dots ,(\\mathbf{x}_{N}, y_{N})$ onde $\\mathbf{x}_i \\in \\mathbb{R}^{n}$ e $y_i \\in \\mathbb{R}$, podemos aproximar a função escondida $f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ (lembrando que $y_i =f(\\mathbf{x}_i)$) por meio de um modelo linear $h$:\n",
    "$$\n",
    "h(\\mathbf{x}_{i}; \\mathbf{w}, b) = \\mathbf{w}^\\top  \\mathbf{x}_{i} + b\n",
    "$$\n",
    "\n",
    "Note que $h(\\mathbf{x}_{i}; \\mathbf{w}, b)$ é na verdade uma [transformação afim](https://en.wikipedia.org/wiki/Affine_transformation) de $\\mathbf{x}_{i}$. Como em outros lugares, vamos usar o termo \"linear\" também para caracterizar uma transformação afim.\n",
    "\n",
    "A saída de $h$ é uma transformação linear de $\\mathbf{x}_{i}$. Usamos a notação $h(\\mathbf{x}_{i}; \\mathbf{w}, b)$ para deixar claro que $h$ é um modelo parametrizado, i.e., a transformação $h$ é definida pelos parâmetros $\\mathbf{w}$ e $b$. Podemos pensar no vetor $\\mathbf{w}$ como um vetor de *pesos* controlando o efeito de cada *feauture* na predição.\n",
    "\n",
    "Adicionando uma feature a mais na obsevação $\\mathbf{x}_{i}$ (com o valor 1) podemos simplificar a notação do modelo:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}_{i}; \\mathbf{w}) = \\hat{y}_{i} = \\mathbf{w}^\\top  \\mathbf{x}_{i}\n",
    "$$\n",
    "\n",
    "Procuramos os melhores parametros $\\mathbf{w}$ de modo que a predição $\\hat{y}_{i}$ seja a mais próxima de $y_{i}$ de acordo com alguma métrica de erro. Usando o *erro quadrárico médio* como tal métrica podemos obter a seguinte função de custo:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Desse modo, a tarefa de achar os parametros $\\mathbf{w}$ se torna a tarefa de encontrar os valores de $\\mathbf{w}$ para minimizar $J(\\mathbf{w})$.\n",
    "\n",
    "**Aqui vamos começar a explorar esse modelo olhando para um dataset bem simples**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "from util import get_hausing_prices_data, plot_points_regression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O dataset\n",
    "\n",
    "Os dados que vamos trabalhar vão ser dados artificiais. Vamos pegar 100 observações com apenas uma *feature* (metros quadrados de um imóvel) e com isso vamos associar um valor (o preço desse imóvel em $). Nossa tarefa vai ser em contruir um modelo que consiga predizer o valor de imóveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_hausing_prices_data(N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotando os dados\n",
    "\n",
    "Acima temos algumas informações sobre os dados, podemos também visualizar cada ponto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points_regression(X,\n",
    "                       y,\n",
    "                       title='Real estate prices prediction',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equações normais\n",
    "\n",
    "Dados $f:\\mathbb{R}^{n\\times m} \\rightarrow \\mathbb{R}$ e $\\mathbf{A} \\in \\mathbb{R}^{n\\times m}$ nos definimos a derivada de $f$ com respeito a $\\mathbf{A}$ como:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_{\\mathbf{A}}f =  \\begin{vmatrix}\n",
    "\\frac{\\partial f}{\\partial \\mathbf{A}_{1,1}} & \\dots & \\frac{\\partial f}{\\partial \\mathbf{A}_{1,m}} \\\\\n",
    "\\vdots &  \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial \\mathbf{A}_{n,1}} &  \\dots & \\frac{\\partial f}{\\partial \\mathbf{A}_{n,m}}\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Seja $\\mathbf{X} \\in \\mathbb{R}^{N\\times n}$ a matriz cujas linhas são as observações do dataset (também chamada de *design matrix*) e seja $\\mathbf{y} \\in \\mathbb{R}^{N}$ o vetor contendo todos os valores de $y_{i}$ (i.e., $\\mathbf{X}_{i,:} = \\mathbf{x}_{i}$ e $\\mathbf{y}_{i} =y_{i}$). É fácil checar que: \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Usando certos conceitos básicos de derivada com matrizes podemos chegar no gradiente de $J(\\mathbf{w})$ com respeito a $\\mathbf{w}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\frac{2}{N} (\\mathbf{X}^{T}\\mathbf{X}\\mathbf{w} -\\mathbf{X}^{T}\\mathbf{y})   \n",
    "\\end{equation}\n",
    "\n",
    "Assim, quando $\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = 0$ temos que \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{T}\\mathbf{X}\\mathbf{w} = \\mathbf{X}^{T}\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Desse modo,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "A solução dada por essas equações são conhecidas como **equações normais**. Note que esse tipo de solução tem um custo, pois conforme cresce o número de variáveis, o tempo da inversão da matriz fica proibitivo. Vale a pena ler [esse material](http://cs229.stanford.edu/notes/cs229-notes1.pdf) para ver o argumento com mais detalhes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "Implemente a predição usando o método de equações normais. Usando apenas a biblioteca **numpy** voce deve completar a função abaixo de modo a adicionar uma *feature* (com apenas 1s) ao dataset e realizar a computação descrita acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation_prediction(X, y):\n",
    "    \"\"\"\n",
    "    Calculates the prediction using the normal equation method.\n",
    "    You should add a new row with 1s.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.array\n",
    "    :param y: regression targets\n",
    "    :type y: np.array\n",
    "    :return: prediction\n",
    "    :rtype: np.array\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "try:\n",
    "    prediction = normal_equation_prediction(X, y) \n",
    "    plot_points_regression(X,\n",
    "                           y,\n",
    "                           title='Real estate prices prediction',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           legend=True)\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
