{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão linear 2:  gradiente descendente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizado e otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada uma função *target* desconhecida $f:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ e uma hipótese $h_{\\mathbf{w}}:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ o erro de $h_{\\mathbf{w}}$ é definido por:\n",
    "\n",
    "\\begin{equation}\n",
    "E_{out}(h_{\\mathbf{w}}) = \\mathbb{E}_{\\mathbf{x}\\sim p_{data}}L(h(\\mathbf{x}; \\mathbf{w}), \\; f(\\mathbf{x}))\n",
    "\\end{equation}\n",
    "\n",
    "em que $p_{data}$ é a distribuição geradora dos dados, $L$  é alguma função de perda (por exemplo, o quadrado da diferença) e $h(\\mathbf{x}; \\mathbf{w}) = h_{\\mathbf{w}}(\\mathbf{x})$. Se tivéssemos acesso a $p_{data}$ poderíamos calcular a função acima para qualquer $h_{\\mathbf{w}}$ e escolher aquele com erro mínimo.\n",
    "\n",
    "Como não temos acesso a $p_{data}$, define-se\n",
    "\n",
    "\\begin{equation}\n",
    "E_{in}(h_{\\mathbf{w}}) = J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $N$ é o tamanho do dataset de treinamento e $y_{i} = f(\\mathbf{x}_{i})$.\n",
    "\n",
    "Como visto em aula, uma relação entre $E_{in}$ e $E_{out}$ pode ser estabelecida pela **Inequação de Hoeffding**.\n",
    "\n",
    "Aqui estamos interessados em encontrar $h_{\\mathbf{w}}$ com erro $J(\\mathbf{w})$ mínimo. Isto corresponde a determinar o ponto mínimo da função $J(\\mathbf{w})$. Por isso, vamos nos concentrar apenas em uma técnica de otimização para minimizar $E_{in}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente ascendente e gradiente descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados $\\mathbf{w}, \\mathbf{u} \\in \\mathbb{R}^{d}$ e $J:\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ tal que $||\\mathbf{u}||_{2} = 1$ a taxa de variação de $J$ no ponto $\\mathbf{w}$ em direção a $\\mathbf{u}$ é chamada de **derivada direcional**, $D_{\\mathbf{u}}J(\\mathbf{w})$. Definindo $g(h) = J(\\mathbf{w} + h\\mathbf{u})$, podemos usar a regra da cadeia e a definição de produto escalar para mostrar que  \n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\mathbf{u}}J(\\mathbf{w}) \\;\\;=\\;\\; \\left. \\frac{dg}{dh} \\right|_{h=0} \\;\\;=\\;\\; \\mathbf{u}^{T}\\nabla_{\\mathbf{w}}J(\\mathbf{w})\n",
    "    \\;\\;=\\;\\; ||\\mathbf{u}||_{2}||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta \\;\\;=\\;\\; ||\\nabla_{\\mathbf{w}}J(\\mathbf{w})||_{2}cos\\theta\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\theta$ é o ângulo entre $\\mathbf{u}$ e $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Assim, pensando em $D_{\\mathbf{u}}J(\\mathbf{w})$ em termos do vetor $\\mathbf{u}$ temos que: $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor máximo quando $\\mathbf{u}$ tem a mesma direção que $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=0$); similarmente $D_{\\mathbf{u}}J(\\mathbf{w})$ tem o valor mínimo quando $\\mathbf{u}$ tem a direção oposta a $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ ($\\theta=\\pi$).\n",
    "\n",
    "Desse modo podemos maximizar $J$ alterando $\\mathbf{w}$ na direção de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ e podemos minimizar $J$ alterando $\\mathbf{w}$ na direção de $-\\nabla_{\\mathbf{w}}J(\\mathbf{w})$. Isso permite dois métodos bem simples de otimização:\n",
    "\n",
    "**Gradiente Ascendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) + \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "    \n",
    "\n",
    "**Gradiente Descendente**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "\n",
    "Em ambos os casos o parâmetro $\\eta \\in \\mathbb{R}_{\\geq}$ é chamado de **taxa de aprendizado** (*learning rate*); ele pondera o tamanho de cada atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   A seguir são apresentados uma sequência de exercícios que ilustram diferentes aspectos práticos na implementação do algoritmo *gradient descent*  \n",
    "Iremos considerar o problema de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all imports\n",
    "import numpy as np\n",
    "import time\n",
    "from util import r_squared, randomize_in_place, get_housing_prices_data\n",
    "from plots import simple_step_plot, plot_points_regression, plot_cost_function_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Vamos usar o mesmo dataset de antes, mas agora vamos dividir os dados em treinamento, validação e teste.\n",
    "Essa divisão é comumente realizada na prática: os dados de treinamento são usados na otimização da função custo $J$, os dados de validação são usados para aferir a qualidade da otimização, e os dados de teste são usados para aferir a qualidade da predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividindo os dados em treinamento, validação e teste\n",
      "\n",
      "train_X shape = (250, 1)\n",
      "\n",
      "train_y shape = (250, 1)\n",
      "\n",
      "valid_X shape = (50, 1)\n",
      "\n",
      "valid_y shape = (50, 1)\n",
      "\n",
      "test_X shape = (50, 1)\n",
      "\n",
      "test_y shape = (50, 1)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_housing_prices_data(N=350, verbose=False)\n",
    "randomize_in_place(X, y)\n",
    "\n",
    "train_X = X[0:250]\n",
    "train_y = y[0:250]\n",
    "valid_X = X[250:300]\n",
    "valid_y = y[250:300]\n",
    "test_X = X[300:]\n",
    "test_y = y[300:]\n",
    "\n",
    "print(\"\\nDividindo os dados em treinamento, validação e teste\")\n",
    "print(\"\\ntrain_X shape = {}\".format(train_X.shape))\n",
    "print(\"\\ntrain_y shape = {}\".format(train_y.shape))\n",
    "print(\"\\nvalid_X shape = {}\".format(valid_X.shape))\n",
    "print(\"\\nvalid_y shape = {}\".format(valid_y.shape))\n",
    "print(\"\\ntest_X shape = {}\".format(test_X.shape))\n",
    "print(\"\\ntest_y shape = {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_points_regression(train_X,\n",
    "                       train_y,\n",
    "                       title='Training data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para minimizar a função de custo vamos ter que colocar os dados em uma outra escala\n",
    "\n",
    "Um modo de fazer isso é o chamado [standard/z score](https://en.wikipedia.org/wiki/Standard_score).\n",
    "Aplicamos a seguinte transformação: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{\\top}_{i} \\leftarrow \\frac{\\mathbf{X}^{\\top}_{i} - \\mu_{i}}{\\sigma_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "em que $\\mathbf{X}^{T}_{i} \\in \\mathbb{R}^{N}$ ($i = 1, \\dots, d$) é um vetor de features da design matrix $\\mathbf{X}$, $\\mu_{i}$ é a média de tal vetor, e $\\sigma_{i}$ seu desvio padrão.\n",
    "\n",
    "A importância de se fazer essa transformação é discutida mais ao final deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 1)** \n",
    "Use a biblioteca numpy para implementar a função que altera os dados conforme a equação acima (essa função deve funcionar para uma design matrix $\\mathbf{X}$ com um número arbitrário de features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Returns standardized version of the ndarray 'X'.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: standardized array\n",
    "    :rtype: np.ndarray(shape=(N, d))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    toy_X = np.array([[1100.3, 2.4, 34.34],\n",
    "                      [2300.3, 1.4, 442.23]])\n",
    "    toy_y = np.array([[1000.2], [2000.5]])\n",
    "    toy_X_norm = standardize(toy_X)\n",
    "    toy_y_norm = standardize(toy_y)\n",
    "    xmean, xstd = np.mean(toy_X_norm), np.std(toy_X_norm)\n",
    "    ymean, ystd = np.mean(toy_y_norm), np.std(toy_y_norm)\n",
    "    assert -1 <= xmean < 0\n",
    "    assert 0 <= ymean < 1\n",
    "    assert 0.9 <= xstd <= 1\n",
    "    assert 0.9 <= ystd <= 1\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_X_norm = standardize(train_X)\n",
    "    train_y_norm = standardize(train_y)\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X), np.std(train_X)\n",
    "    xmax, xmin = np.max(train_X), np.min(train_X)\n",
    "    ymean, ystd = np.mean(train_y), np.std(train_y)\n",
    "    ymax, ymin = np.max(train_y), np.min(train_y)\n",
    "\n",
    "    print(\"Dados originais\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "\n",
    "\n",
    "    xmean, xstd = np.mean(train_X_norm), np.std(train_X_norm)\n",
    "    xmax, xmin = np.max(train_X_norm), np.min(train_X_norm)\n",
    "    ymean, ystd = np.mean(train_y_norm), np.std(train_y_norm)\n",
    "    ymax, ymin = np.max(train_y_norm), np.min(train_y_norm)\n",
    "\n",
    "    print(\"Dados normalizados\\n\")\n",
    "    print(\"X:\\nmean {}, std {:.2f}, max {}, min {}\".format(xmean,\n",
    "                                                           xstd,\n",
    "                                                           xmax,\n",
    "                                                           xmin))\n",
    "    print(\"\\ny:\\nmean {}, std {:.2f}, max {}, min {}\\n\".format(ymean,\n",
    "                                                             ystd,\n",
    "                                                             ymax,\n",
    "                                                             ymin))\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$')\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Adicionando uma componente com apenas 1s como uma nova feature\n",
    "Conforme já vimos, adicionar uma componente (coordenada artificial) constante 1 é conveniente. Isto é, em vez de $\\mathbf{x} \\in \\mathbb{R}^d$ é conveniente considerarmos $(1,\\mathbf{x}) \\in \\mathbb{R}^{d+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "def add_feature_ones(X):\n",
    "    \"\"\"\n",
    "    Returns the ndarray 'X' with the extra\n",
    "    feature column containing only 1s.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: output array\n",
    "    :rtype: np.ndarray(shape=(N, d+1))\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "\n",
    "try:\n",
    "    train_X_1 = add_feature_ones(train_X_norm)\n",
    "    print(\"\\ntrain_X shape = {}\".format(train_X_1.shape))\n",
    "\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Criando a predição da regressão linear e plotando uma predição arbitrária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "def linear_regression_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Calculates the linear regression prediction.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: prediction\n",
    "    :rtype: np.array(shape=(N, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    return X.dot(w)\n",
    "\n",
    "try:\n",
    "    w = np.array([[1.2], [2.3]])\n",
    "    prediction = linear_regression_prediction(train_X_1, w)\n",
    "\n",
    "    plot_points_regression(train_X_norm,\n",
    "                           train_y_norm,\n",
    "                           title='Training data (normalized)',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           legend=True)\n",
    "except NameError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computando a função de custo\n",
    "\n",
    "Usando o erro quadrárico médio, a função de custo $J(\\mathbf{w})$ para a tarefa de regressão linear pode ser escrita de dois modos. A forma iterativa:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "e a forma vetorial:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 2)**  \n",
    "Use a biblioteca numpy para implementar a função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates  mean square error cost.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: cost\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    toy_w = np.array([[1], [1], [2]])\n",
    "    toy_X = np.array([[2, 3, 1],\n",
    "                      [5, 1, 2]])\n",
    "    toy_y = np.array([[1], [1]])\n",
    "    assert compute_cost(toy_X, toy_y, toy_w) == 58.5\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos olhar a superficie de custo e ver onde se situa um valor $J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    initial_w = np.array([[15], [-35.3]])\n",
    "    initial_J = compute_cost(train_X_1, train_y_norm, initial_w)\n",
    "\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\",\n",
    "                             weights_list=[initial_w.flatten()],\n",
    "                             cost_list=[initial_J])\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando os gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fácil calcular a derivada parcial de $J(\\mathbf{w})$ com relação a cada entrada $j$ de $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{j}} = \\frac{2}{N}\\sum_{i=1}^{N} (\\hat{y}_i - y_i) \\mathbf{x}_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Lembre que o gradiente de $J(\\mathbf{w})$ com relação a $\\mathbf{w}$ é:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}J(\\mathbf{w}) = \\begin{bmatrix}\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{1}} \\dots \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_{m}} \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 3)**  \n",
    "Use a biblioteca numpy para calcular $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_wgrad(X, y, w):\n",
    "    \"\"\"\n",
    "    Calculates gradient of J(w) with respect to w.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :return: gradient\n",
    "    :rtype: np.array(shape=(d, 1))\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "def grad_check(X, y, w, h=1e-4):\n",
    "    \"\"\"\n",
    "    Check gradients for linear regression.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param h: small variation\n",
    "    :type h: float\n",
    "    :return: gradient test\n",
    "    :rtype: boolean\n",
    "    \"\"\"\n",
    "    Jw = compute_cost(X, y, w)\n",
    "    grad = compute_wgrad(X, y, w)\n",
    "    passing = True\n",
    "    d = w.shape[0]\n",
    "    for i in range(d):\n",
    "        w_plus_h = np.array(w, copy=True)\n",
    "        w_plus_h[i] = w_plus_h[i] + h\n",
    "        Jw_plus_h = compute_cost(X, y, w_plus_h)\n",
    "        w_minus_h = np.array(w, copy=True)\n",
    "        w_minus_h[i] = w_minus_h[i] - h\n",
    "        Jw_minus_h = compute_cost(X, y, w_minus_h)\n",
    "        numgrad_i = (Jw_plus_h - Jw_minus_h) / (2 * h)\n",
    "        reldiff = abs(numgrad_i - grad[i]) / max(1, abs(numgrad_i), abs(grad[i]))\n",
    "        if reldiff > 1e-5:\n",
    "            passing = False\n",
    "            msg = \"\"\"\n",
    "            Seu gradiente = {0}\n",
    "            Gradiente numérico = {1}\"\"\".format(grad[i], numgrad_i)\n",
    "            print(\"            \" + str(i) + \": \" + msg)\n",
    "            print(\"            Jw = {}\".format(Jw))\n",
    "            print(\"            Jw_plus_h = {}\".format(Jw_plus_h))\n",
    "            print(\"            Jw_minus_h = {}\\n\".format(Jw_minus_h))\n",
    "\n",
    "    if passing:\n",
    "        print(\"Gradiente passando!\")\n",
    "    \n",
    "    return passing \n",
    "\n",
    "try:\n",
    "    toy_w1 = np.array([[1.], [2.], [1.], [2.]])\n",
    "    toy_X1 = np.array([[2., 3., 1., 2.],\n",
    "                      [5., 1., 1., 2.]])\n",
    "    toy_y1 = np.array([[1.], [-1.]])\n",
    "    toy_w2 = np.array([[-100.22], [20002.1], [102.5]])\n",
    "    toy_X2 = np.array([[2111.3, -2223., 404.0],\n",
    "                      [5222., -22221., 3.3]])\n",
    "    toy_y2 = np.array([[122.], [221.]])\n",
    "    toy_w3 = np.array([[-10.22], [-3.1]])\n",
    "    toy_X3 = np.array([[1.3, -1.2],\n",
    "                      [2.2, -2.1],\n",
    "                      [-2.3, -5.5],\n",
    "                      [3.2, 8.1],\n",
    "                      [3.3, -1.1],\n",
    "                      [-3.4, -2.22],\n",
    "                      [2.23, -4.4],\n",
    "                      [5.2, -2.3]])\n",
    "    toy_y3 = np.array([[10.3],\n",
    "                       [23.3],\n",
    "                       [10.1],\n",
    "                       [-20.2],\n",
    "                       [-10.2],\n",
    "                       [20.2],\n",
    "                       [-14.4],\n",
    "                       [-30.3]])\n",
    "    \n",
    "    assert grad_check(toy_X1, toy_y1, toy_w1)\n",
    "    assert grad_check(toy_X2, toy_y2, toy_w2)\n",
    "    assert grad_check(toy_X3, toy_y3, toy_w3)\n",
    "\n",
    "except NotImplementedError:\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "A versão mais simples do algoritmo *gradient descent* faz uso de todas as observações do dataset de treinamento (esse algoritmo também é conhecido como *batch gradient descent* ou *vanilla gradient descent*).\n",
    "\n",
    "**Batch gradient descent**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Compute the gradient $\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$ \n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 4)** \n",
    "Implemente o algoritmo batch gradient descent com a taxa de apreendizado fixa para a regressão linear. A função abaixo deve retornar três coisas: o vetor de pesos $\\mathbf{w}$, uma lista com cada peso obtido ao longo do treinamento, e uma lista com o custo de cada peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, w, learning_rate, num_iters):\n",
    "    \"\"\"\n",
    "     Performs batch gradient descent optimization.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    \n",
    "    weights_history = [w.flatten()]\n",
    "    cost_history = [compute_cost(X, y, w)]\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    learning_rate = 0.8\n",
    "    iterations = 20000\n",
    "    init = time.time()\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1 segundo \")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Agora podemos treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    learning_rate = 0.03\n",
    "    iterations = 400\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                              train_y_norm,\n",
    "                                                              initial_w,\n",
    "                                                              learning_rate,\n",
    "                                                              iterations)\n",
    "    title = \"Optimization landscape\\nlearning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                                                  iterations)\n",
    "    plot_cost_function_curve(train_X_1,\n",
    "                             train_y_norm,\n",
    "                             compute_cost,\n",
    "                             title=title,\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history)\n",
    "    simple_step_plot([cost_history],\n",
    "                 \"loss\",\n",
    "                 'Training loss\\nlearning rate = {} | iterations = {}'.format(learning_rate,\n",
    "                                                                              iterations))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper parâmetros (*hyperparameters*)\n",
    "\n",
    "\n",
    "Hiper parâmetros são parâmetros que controlam o comportamento do algoritmo. Eles não são modificados pelo algoritmo de aprendizado. Escolhemos os hiper parâmetros de acordo com a performance deles no dataset de treinamento. Para evitar que o modelo decore o dataset de treinamento, pegamos uma parte desse dataset só para achar os melhores hiper parâmetros. Essa parte é chamada de **dataset de validação** (*validation set*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    valid_X_norm = standardize(valid_X)\n",
    "    valid_y_norm = standardize(valid_y)\n",
    "    valid_X_1 = add_feature_ones(valid_X_norm)\n",
    "\n",
    "    hyper_params = [(0.001, 200),\n",
    "                    (0.1, 10),\n",
    "                    (0.9, 8),\n",
    "                    (0.02, 600)]\n",
    "\n",
    "    all_costs = []\n",
    "    all_w = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        w, weights_history, cost_history = batch_gradient_descent(train_X_1,\n",
    "                                                                  train_y_norm,\n",
    "                                                                  initial_w,\n",
    "                                                                  learning_rate,\n",
    "                                                                  iterations)\n",
    "        all_costs.append(compute_cost(valid_X_1, valid_y_norm, w))\n",
    "        all_w.append(w)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {} | iterations = {}\".format(learning_rate,\n",
    "                                                               iterations)\n",
    "\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    best_result_i = np.argmin(all_costs)\n",
    "    best_w = all_w[best_result_i]\n",
    "    lowest_cost = all_costs[best_result_i]\n",
    "    best_params = hyper_params[best_result_i]\n",
    "\n",
    "    result_str = \"Best hyperparameters\\n\"\n",
    "    result_str += \"learning rate = {}\".format(best_params[0])\n",
    "    result_str += \" | iterations = {}\\n\".format(best_params[1])\n",
    "    result_str += \"w = {}\\n\".format(best_w.flatten())\n",
    "    result_str += \"lowest validation set cost = {}\\n\".format(lowest_cost)\n",
    "\n",
    "    print(result_str)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Com o modelo treinado e escolhidos os melhores hiper parâmetros, podemos avaliá-lo sobre o dataset de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_X_norm = standardize(test_X)\n",
    "    test_y_norm = standardize(test_y)\n",
    "    test_X_1 = add_feature_ones(test_X_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    prediction = linear_regression_prediction(test_X_1, best_w)\n",
    "    prediction = (prediction * np.std(train_y)) + np.mean(train_y)\n",
    "    r_2 = r_squared(test_y, prediction)\n",
    "\n",
    "    plot_points_regression(test_X,\n",
    "                           test_y,\n",
    "                           title='Test data',\n",
    "                           xlabel=\"m\\u00b2\",\n",
    "                           ylabel='$',\n",
    "                           prediction=prediction,\n",
    "                           r_squared=r_2,\n",
    "                           legend=True)\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente estocástico\n",
    "\n",
    "Nos casos em que $N$ é um número grande, computar $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ a cada iteração se torna algo muito custoso. Uma estratégia para lidar com isso é **aproximar** $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ usando o gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})} = \\nabla_{\\mathbf{w}}\\frac{1}{m}\\sum_{i=1}^{m} L(h(\\mathbf{x}_{i}; \\mathbf{w}), \\; y_{i})\n",
    "\\end{equation}\n",
    "\n",
    "em que $(\\mathbf{x}_{1}, y_{1}), \\dots ,(\\mathbf{x}_{m}, y_{m})$ é uma amostragem aleatória dos dados de treinamento. A  estocasticidade surge da escolha desses $m$ dados (para que $\\hat{\\nabla_{\\mathbf{w}}J(\\mathbf{w})}$ seja um estimador não enviesado de $\\nabla_{\\mathbf{w}}J(\\mathbf{w})$ nós amostramos os $m$ dados a cada iteração). Normalmente usamos o nome **gradiente descendente estocástico** (*stochastic gradient descent* ou *online gradient descent*) quando $m=1$, e usamos o nome **minibatch stochastic gradient descent** quando $1 < m <N$ (nesse caso estamos usando apenas um pequeno lote dos dados, um *minibatch*). Usamos *batch* para referir a um *minibatch*, não confunda isso com *batch gradient descent*.\n",
    "\n",
    "\n",
    "**Stochastic gradient descent (SGD)**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Sample a minibatch of $m$ examples from the training data.\n",
    "    * Compute the gradient estimate $\\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **Exercício 5)** \n",
    "Implemente o algoritmo stochastic gradient descent para a regressão linear com a taxa de apreendizado fixa. A saída da função é a mesma da função do exercício 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, learning_rate, num_iters, batch_size):\n",
    "    \"\"\"\n",
    "     Performs stochastic gradient descent optimization\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param w: weights\n",
    "    :type w: np.array(shape=(d, 1))\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param num_iters: number of iterations\n",
    "    :type num_iters: int\n",
    "    :param batch_size: size of the minibatch\n",
    "    :type batch_size: int\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return w, weights_history, cost_history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste exercício 5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init = time.time()\n",
    "    learning_rate = 0.8\n",
    "    iterations = 2000\n",
    "    batch_size = 36\n",
    "    w, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                   train_y_norm,\n",
    "                                                                   initial_w,\n",
    "                                                                   learning_rate,\n",
    "                                                                   iterations,\n",
    "                                                                   batch_size)\n",
    "    assert cost_history[-1] < cost_history[0]\n",
    "    assert type(w) == np.ndarray\n",
    "    assert len(weights_history) == len(cost_history)\n",
    "    init = time.time() - init\n",
    "    print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "    print(\"Tem que ser em menos de 1.2 segundos\")\n",
    "    \n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Podemos experimentar com diferentes tamanhos de batch para ver que quanto maior o tamanho do batch (mais próximo de $N$) menor a variância."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    hyper_params = [(0.001, 1000, 1),\n",
    "                    (0.001, 1000, 10),\n",
    "                    (0.001, 1000, 36)]\n",
    "    all_costs = []\n",
    "\n",
    "    for param in hyper_params:\n",
    "        learning_rate = param[0]\n",
    "        iterations = param[1]\n",
    "        batch_size = param[2]\n",
    "        _, weights_history, cost_history = stochastic_gradient_descent(train_X_1,\n",
    "                                                                       train_y_norm,\n",
    "                                                                       initial_w,\n",
    "                                                                       learning_rate,\n",
    "                                                                       iterations,\n",
    "                                                                       batch_size)\n",
    "        all_costs.append(cost_history)\n",
    "        title = \"Optimization landscape\\n\"\n",
    "        title += \"learning rate = {}\".format(learning_rate)\n",
    "        title += \" | iterations = {}\".format(iterations)\n",
    "        title += \" | batch size = {}\".format(batch_size)\n",
    "        plot_cost_function_curve(train_X_1,\n",
    "                                 train_y_norm,\n",
    "                                 compute_cost,\n",
    "                                 title=title,\n",
    "                                 weights_list=weights_history,\n",
    "                                 cost_list=cost_history)\n",
    "\n",
    "\n",
    "    _, _, cost_history_full = batch_gradient_descent(train_X_1,\n",
    "                                                     train_y_norm,\n",
    "                                                     initial_w,\n",
    "                                                     learning_rate=0.001,\n",
    "                                                     num_iters=1000)\n",
    "\n",
    "    all_costs.append(cost_history_full)\n",
    "    labels_size = [\"batch size = \" + str(param[2]) for param in hyper_params]\n",
    "    labels_size += [\"batch size = \" + str(train_X_1.shape[0])]\n",
    "\n",
    "    simple_step_plot(all_costs,\n",
    "                     \"loss\",\n",
    "                     'Training loss',\n",
    "                      figsize=(8, 8),\n",
    "                      labels=labels_size)\n",
    "\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que normalizar?\n",
    "\n",
    "O primeiro motivo para se normalizar os dados é para evitar *overflow*. Também é verdade que quando não normalizamos os dados as *features* podem apresentar diferentes escalas -- note que esse é o caso nesse dataset em que uma *feature* só tem $1$s e a outra ($m^{2}$) apresenta bastante variação. Isso influencia no gradiente de modo que a cada atualização os valores dos pesos vão mudar de modo diferente mesmo usando o mesmo *learning rate*.\n",
    "\n",
    "Isso pode ser visto quando acompanhamos a mudança nos pesos ao longo do treinamento no dataset original e no normalizado. Note como o parâmetro $\\mathbf{w}[1]$ (que pondera a feature ($m^{2}$)) muda bem mais que o parâmetro $\\mathbf{w}[0]$ quando usamos o dataset não normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    _, weights_history_norm, cost_history_norm = batch_gradient_descent(train_X_1,\n",
    "                                                                        train_y_norm,\n",
    "                                                                        initial_w,\n",
    "                                                                        learning_rate,\n",
    "                                                                        10)\n",
    "\n",
    "    train_X_1_non_norm = add_feature_ones(train_X)\n",
    "\n",
    "    w, weights_history, cost_history = batch_gradient_descent(train_X_1_non_norm,\n",
    "                                                              train_y,\n",
    "                                                              initial_w,\n",
    "                                                              0.000002,\n",
    "                                                              10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist_norm = [w[0] for w in weights_history_norm]\n",
    "    w1_hist_norm = [w[1] for w in weights_history_norm]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist_norm), np.std(w0_hist_norm), np.max(w0_hist_norm), np.min(w0_hist_norm)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w0_hist_norm), np.std(w1_hist_norm), np.max(w1_hist_norm), np.min(w1_hist_norm)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))              \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    w0_hist = [w[0] for w in weights_history]\n",
    "    w1_hist = [w[1] for w in weights_history]\n",
    "\n",
    "\n",
    "    w0mean, w0sdt, w0max, w0min = np.mean(w0_hist), np.std(w0_hist), np.max(w0_hist), np.min(w0_hist)\n",
    "    w1mean, w1sdt, w1max, w1min = np.mean(w1_hist), np.std(w1_hist), np.max(w1_hist), np.min(w1_hist)\n",
    "\n",
    "    print(\"\\nVariação dos pesos com o dataset não normalizado\\n\")\n",
    "    print(\"w[0]:\\nmean {}, std {:.2f}, max {}, min {}\".format(w0mean, w0sdt, w0max, w0min))\n",
    "    print(\"w[1]:\\nmean {}, *std {:.2f}*, max {}, min {}\".format(w1mean, w1sdt, w1max, w1min))\n",
    "\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Um dos resultados dessa atualização em scala diferente para cada feature é a não convergência do algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falta fazer!\n"
     ]
    }
   ],
   "source": [
    "try:   \n",
    "    simple_step_plot([cost_history_norm],\n",
    "                     \"loss\",\n",
    "                     'Training loss (normalized)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    simple_step_plot([cost_history],\n",
    "                     \"loss\",\n",
    "                     'Training loss (non normalized)')\n",
    "\n",
    "\n",
    "\n",
    "    plot_cost_function_curve(train_X_1_non_norm,\n",
    "                             train_y,\n",
    "                             compute_cost,\n",
    "                             title=\"Optimization landscape\\n(non normalized data)\",\n",
    "                             weights_list=weights_history,\n",
    "                             cost_list=cost_history,\n",
    "                             range_points=(100, 100))\n",
    "except (NotImplementedError, NameError):\n",
    "    print(\"Falta fazer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais otimização!\n",
    "\n",
    "Há muitos outros algoritmos de otimização construídos em cima da ideia de gradiente descendente. Um bom resumo de alguns desses algoritimos pode ser encontrado [aqui](http://ruder.io/optimizing-gradient-descent/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
